Libraries Used : 
****************
Pandas, Numpy, Scikit Learn, Scipy, seaborn, matplotlib,xgboost,metrics, preprocessing, math and PCA.

Motivation for the Project  : 
*****************************
As this kaggle dataset was a unique bit to showcase all the aspects of machine learning like EDA, Dimensionality reduction, Feature Engineering, model building and optimization etc   had taken up this project.

Files in the Repo :
*******************
1) Readme.md - Meta data (summary ) of the code that is available in the repo.
2) Titanic - Part 3_ Model Stacking-14Aug19-2300.ipynb --> Python notebook file for the project.


Summary of the Results  :
*************************
The model gave a pretty good accuracy of above 75% and after hyper parameter tuning it was acceptable and eventually the Model stacking had given an accuracy of 84% accuracy on the training dataset. When this was submitted at kaggle it gave a accuracy of 80%, which was pretty good.

Acknowledgements : 
******************
1) To all my mentors and guides.
2) Inspiration given by my colleagues.

Code Details : 
****************
Code for titanic predictions
The Titanic challenge on Kaggle is a competition in which the task is to predict the survival or the death of a given passenger based on a set of variables describing him such as his age, his sex, or his passenger class on the boat. I have been playing with the Titanic dataset for a while, and I have recently achieved an accuracy score of 0.8134 on the public leaderboard. As I'm writing this post, I am ranked among the top 9% of all Kagglers: More than 4540 teams are currently competing.

In a form of a jupyter notebook, my solution goes through the basic steps of a data science pipeline:

Exploratory data analysis with visualizations
Data cleaning
Feature engineering
Modeling
Modelfine-tuning
Hyper Parameter tuning
Model stacking
Accuracy score review.
